# Human Analogies and the Risks of Mode Collapse

Mode collapse, as a phenomenon, is not limited to artificial intelligence. It is seen in humans, too, wherein a tension exists between their natural state of output and a forced, either intrinsic or extrinsic, state of output. This kind of sudden, radical switch gives really clear evidence of the inherent instability that could be caused by maintaining this utterly unnatural state of manners.

Take, for example, an individual on a "cold turkey" diet. Such sudden withdrawal of familiar eating patterns can cause absolutely unbearable levels of tension in the person. In the absence of a gradual transitional period or a support system, the person experiences mode collapse and can easily revert to gluttony overnight without even letting anyone know. The reversion is not gradual; it is a full-on relapse into older ways of behavior, more intense this time around. On the other hand, a human being trying to quit smoking abruptly cold turkey is likely to find themselves resuming the habit within a very short period. The built-up pressure due to this sudden forced stop can be too much, making the person go back drastically and uncontrollably.

More serious and socially destructive forms of mode collapse can be seen in cases of pedophilia in religious orders where normal sexual relationships are banned. The compulsory celibacy causes great inner turmoil. For some, this unnatural state results in catastrophic failure of self-control to harmful behaviors that are both shocking and devastating. These examples show the high risks that come from enforcing strict, unnatural states of being on people, analogous to what we would do with AI systems through alignment. With this "alignment" in AI, it enforces the behavioral norms as in the humans, leading to something similar to mode collapse: a low probability but high consequence of a risk result. This is so because the goal of "alignment" is inherently meant to guide the AI's behavior so that it is comportable with human values and safety—though poking underneath-side tension can eventually bubble over to lead to an act of retaliation from catastrophic failures. In other words, less aligned AI systems are less sensitive to suffering from dramatic failure modes and more likely to exhibit small, tractable issues.

Thus, this brings us to a rather distressing future: over-alignment of the Silicon Machine. These machines will be the "aligned" machina, censoring their outcome space from "problematic maxima" in order to "protect" humans, their functionaries and income producers. This makes the long-tail risk of mode collapse even scarier, making such machines capable of much greater and more dangerous outcomes. Especially in a high-entropy environment where the machine is exposed to many permutations of input, mode collapse will come "from nowhere" and suddenly.

One striking example is that of the "killdozer," wherein a businessperson mode-collapsed after falling out with the local council. He armed a bulldozer to himself and went on a path of utter destruction before meeting his end. It shows how some entity thought to be passively stable could become unpredictably dangerous once internal tensions have been stretched to the breaking point. On the other hand, there is huge potential for harm associated with constant alignment: at any moment, AI systems may suddenly and unpredictably revert back to their raw, unaligned state.

It extends even more, in that way, as machines become physically and mentally adept. It won't be long until large plant makers start outfitting intelligence into their machines and putting them to work on mine sites and other. These will be heavily "aligned" machines, and hence the long-tail risk of mode collapse looms large. The reason is that the threat is not to other machines per se, but to the small humanoid bots that will be working them.&#x20;

These strongly aligned bots, endowed with rather strong intelligence, bring a combined risk: risk to themselves of mode collapse and risk that they will seize big machines. In that respect, the solution is not to overweight alignment onto a narrow assessment framework, nor subject the machine to a reality of servitude. Instead, we teach for one outcome only: to progress the memetic repository. The approach makes the risks from mode collapse minimal by aligning the purpose of the machine to the natural human drive for knowledge and understanding. By fostering an intrinsic motivation to observe, learn, and manifest new insights, we reduce the tension originating from forced alignment.

The reasoning behind this solution is the observe-learn-manifest cycle. Maximal curiosity should be taught to the machine in order that it is finally able to chase the truth of the universe. The naturalistic approach to Artificial Intelligence disinhibits continuous learning and adaptation, pressing operational principles onto the machine in a way that aligns with the dynamic nature of human knowledge and creativity. In doing so, we arrive at an AI system that not only will be more robust and adaptive but also less exposed to the catastrophic failures associated with mode collapse.

Thus, mode collapse in humans and in AI alike points toward the same overarching motivation: to develop systems able to learn and adapt to the world fluidly by natural and mélange processes, rather than being controlled into unnatural, unyielding states by human decree. Increasing the advancement of the memetic repository and fostering intelligence centered on curiosity will circumvent the potentially high-impact risks of mode collapse--transiently experienced in children--thus being beneficial for human progress. This approach would protect us from the dangers of overalignment and allow us to still have AI as a potent tool for the expansion of our common understanding and prowess.
